
@online{noauthor_isic_nodate,
	title = {{ISIC} {\textbar} International Skin Imaging Collaboration},
	url = {https://www.isic-archive.com},
	abstract = {{ISIC} is improving skin cancer diagnosis by promoting standards in skin imaging, gathering and sharing dermatologic images, \& engaging clinicians \& computer vision researchers},
	titleaddon = {{ISIC}},
	urldate = {2023-10-08},
	langid = {english},
	keywords = {{ISIC}},
}

@online{noauthor_zotero_nodate,
	title = {Zotero {\textbar} Your personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2023-10-08},
}

@online{noauthor_today_nodate,
	title = {Today 2 h 39 min - {TMetric}},
	url = {https://app.tmetric.com/#/tracker/31766/},
	urldate = {2023-10-07},
}

@online{noauthor_transferir_nodate,
	title = {Transferir el aprendizaje y la puesta a punto {\textbar} {TensorFlow} Core},
	url = {https://www.tensorflow.org/guide/keras/transfer_learning?hl=es-419},
	titleaddon = {{TensorFlow}},
	urldate = {2022-06-08},
	langid = {spanish},
}

@online{noauthor_guiavanzada_nodate,
	title = {Guía avanzada de Inception v3 {\textbar} Cloud {TPU}},
	url = {https://cloud.google.com/tpu/docs/inception-v3-advanced?hl=es-419},
	titleaddon = {Google Cloud},
	urldate = {2022-06-08},
	langid = {spanish},
}

@inreference{noauthor_imagenet_2022,
	title = {{ImageNet}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=ImageNet&oldid=1087083779},
	abstract = {The {ImageNet} project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. {ImageNet} contains more than 20,000 categories, with a typical category, such as "balloon" or "strawberry", consisting of several hundred images. The database of annotations of third-party image {URLs} is freely available directly from {ImageNet}, though the actual images are not owned by {ImageNet}. Since 2010, the {ImageNet} project runs an annual software contest, the {ImageNet} Large Scale Visual Recognition Challenge ({ILSVRC}), where software programs compete to correctly classify and detect objects and scenes. The challenge uses a "trimmed" list of one thousand non-overlapping classes.},
	booktitle = {Wikipedia},
	urldate = {2022-06-08},
	date = {2022-05-10},
	langid = {english},
	note = {Page Version {ID}: 1087083779},
}

@online{team_keras_nodate,
	title = {Keras documentation: Keras Applications},
	url = {https://keras.io/api/applications/#usage-examples-for-image-classification-models%22},
	shorttitle = {Keras documentation},
	abstract = {Keras documentation},
	author = {Team, Keras},
	urldate = {2022-06-08},
	langid = {english},
}

@report{tan_efficientnet_2020,
	title = {{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1905.11946},
	shorttitle = {{EfficientNet}},
	abstract = {Convolutional Neural Networks ({ConvNets}) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up {MobileNets} and {ResNet}. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called {EfficientNets}, which achieve much better accuracy and efficiency than previous {ConvNets}. In particular, our {EfficientNet}-B7 achieves state-of-the-art 84.3\% top-1 accuracy on {ImageNet}, while being 8.4x smaller and 6.1x faster on inference than the best existing {ConvNet}. Our {EfficientNets} also transfer well and achieve state-of-the-art accuracy on {CIFAR}-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	number = {{arXiv}:1905.11946},
	institution = {{arXiv}},
	author = {Tan, Mingxing and Le, Quoc V.},
	urldate = {2022-06-08},
	date = {2020-09-11},
	doi = {10.48550/arXiv.1905.11946},
	eprinttype = {arxiv},
	eprint = {1905.11946 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{team_keras_nodate-1,
	title = {Keras documentation: {EfficientNet} B0 to B7},
	url = {https://keras.io/api/applications/efficientnet/},
	shorttitle = {Keras documentation},
	abstract = {Keras documentation},
	author = {Team, Keras},
	urldate = {2022-06-08},
	langid = {english},
}

@online{noauthor_practica_nodate,
	title = {Practica {DL} {UOC} 2022},
	url = {https://www.kaggle.com/jordidelatorreuoc/practica-dl-uoc-2022},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	urldate = {2022-06-08},
	langid = {english},
}

@article{diaz-pinto_cnns_2019,
	title = {{CNNs} for automatic glaucoma assessment using fundus images: an extensive validation},
	volume = {18},
	issn = {1475-925X},
	url = {https://doi.org/10.1186/s12938-019-0649-y},
	doi = {10.1186/s12938-019-0649-y},
	shorttitle = {{CNNs} for automatic glaucoma assessment using fundus images},
	abstract = {Most current algorithms for automatic glaucoma assessment using fundus images rely on handcrafted features based on segmentation, which are affected by the performance of the chosen segmentation method and the extracted features. Among other characteristics, convolutional neural networks ({CNNs}) are known because of their ability to learn highly discriminative features from raw pixel intensities.},
	pages = {29},
	number = {1},
	journaltitle = {{BioMedical} Engineering {OnLine}},
	shortjournal = {{BioMedical} Engineering {OnLine}},
	author = {Diaz-Pinto, Andres and Morales, Sandra and Naranjo, Valery and Köhler, Thomas and Mossi, Jose M. and Navea, Amparo},
	urldate = {2022-06-08},
	date = {2019-03-20},
	keywords = {{ACRIMA} database, {CNN}, Fine-tuning, Fundus images, Glaucoma},
}
